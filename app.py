import requests
import json
import subprocess
import hashlib
import copy
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import logging
import time


class N8nMonitor:
    """n8n å·¥ä½œæµç¨‹ç›£æ§èˆ‡å‚™ä»½ç³»çµ±"""

    def __init__(self, config_path: str = 'config.json'):
        self.load_config(config_path)
        self.setup_logging()
        self.last_health_status = None

    def load_config(self, config_path: str):
        """è¼‰å…¥è¨­å®šæª”"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        self.n8n_url = config['n8n']['url'].rstrip('/')
        self.api_key = config['n8n']['api_key']
        self.git_repo_path = Path(config['git']['repo_path'])
        self.git_remote_url = config['git'].get('remote_url', 'https://github.com/guyu1010/wanin_n8n_bk_data')
        self.notifications = config.get('notifications', {})
        self.schedule_config = config.get('schedule', {
            'enabled': False,
            'interval': 600,
            'run_on_startup': True
        })

        self.headers = {
            'X-N8N-API-KEY': self.api_key,
            'Accept': 'application/json'
        }
        self.timeout = config.get('timeout', 10)
        self.max_retries = config.get('max_retries', 3)

    def setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('n8n_monitor.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    # ========== å¥åº·æª¢æŸ¥ ==========

    def check_health(self) -> Dict:
        """æª¢æŸ¥ n8n å¥åº·ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.n8n_url}/healthz", timeout=self.timeout)

            if response.status_code == 200:
                return {
                    'status': 'healthy',
                    'response_time': response.elapsed.total_seconds(),
                    'timestamp': datetime.now().isoformat()
                }
            else:
                return {
                    'status': 'unhealthy',
                    'error': f'HTTP {response.status_code}',
                    'timestamp': datetime.now().isoformat()
                }

        except requests.exceptions.Timeout:
            return {'status': 'timeout', 'error': 'Connection timeout', 'timestamp': datetime.now().isoformat()}
        except requests.exceptions.ConnectionError as e:
            return {'status': 'down', 'error': str(e), 'timestamp': datetime.now().isoformat()}
        except Exception as e:
            return {'status': 'error', 'error': str(e), 'timestamp': datetime.now().isoformat()}

    def handle_health_change(self, health_status: Dict):
        """è™•ç†å¥åº·ç‹€æ…‹è®Šæ›´"""
        current_status = health_status['status']

        if self.last_health_status != current_status:
            if current_status != 'healthy':
                self.logger.error(f"âœ— n8n æœå‹™ç•°å¸¸: {current_status}")
                self.send_webhook_notification({
                    'title': 'n8n æœå‹™ç•°å¸¸',
                    'status': 'error',
                    'health_status': health_status
                })
            else:
                self.logger.info("âœ“ n8n æœå‹™å·²æ¢å¾©æ­£å¸¸")
                self.send_webhook_notification({
                    'title': 'n8n æœå‹™æ¢å¾©',
                    'status': 'success',
                    'health_status': health_status
                })
            self.last_health_status = current_status

    # ========== å·¥ä½œæµç¨‹æ“ä½œ ==========

    def get_all_workflows(self) -> Optional[List[Dict]]:
        """å–å¾—æ‰€æœ‰å·¥ä½œæµç¨‹ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰"""
        url = f"{self.n8n_url}/api/v1/workflows"

        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                return response.json()['data']
            except Exception as e:
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    self.logger.error(f"âœ— ç„¡æ³•å–å¾—å·¥ä½œæµç¨‹: {e}")
                    return None

    def get_workflow_detail(self, workflow_id: str) -> Optional[Dict]:
        """å–å¾—å·¥ä½œæµç¨‹è©³ç´°å…§å®¹"""
        try:
            response = requests.get(
                f"{self.n8n_url}/api/v1/workflows/{workflow_id}",
                headers=self.headers,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json()
        except Exception:
            return None

    def calculate_hash(self, workflow_data: Dict) -> str:
        """è¨ˆç®—å·¥ä½œæµç¨‹çš„ hash å€¼ï¼ˆåƒ…é—œæ³¨åŠŸèƒ½æ€§è®Šæ›´ï¼‰"""
        clean_data = copy.deepcopy(workflow_data)

        # ç§»é™¤ä¸å½±éŸ¿åŠŸèƒ½çš„æ¬„ä½
        for field in ['updatedAt', 'createdAt', 'versionId', 'id']:
            clean_data.pop(field, None)

        # ç§»é™¤ nodes ä¸­çš„ä½ç½®è³‡è¨Š
        if 'nodes' in clean_data:
            for node in clean_data['nodes']:
                node.pop('position', None)

        content = json.dumps(clean_data, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

    def _analyze_workflow_changes(self, old_workflow: Dict, new_workflow: Dict) -> Dict:
        """åˆ†æå·¥ä½œæµç¨‹çš„è®Šæ›´"""
        changes = {'added_nodes': [], 'removed_nodes': [], 'modified_nodes': []}

        old_nodes = {node['id']: node for node in old_workflow.get('nodes', [])}
        new_nodes = {node['id']: node for node in new_workflow.get('nodes', [])}

        # æ–°å¢çš„ç¯€é»
        for node_id, node in new_nodes.items():
            if node_id not in old_nodes:
                changes['added_nodes'].append(
                    f"{node.get('name', 'Unknown')} ({node.get('type', 'Unknown').split('.')[-1]})"
                )

        # åˆªé™¤çš„ç¯€é»
        for node_id, node in old_nodes.items():
            if node_id not in new_nodes:
                changes['removed_nodes'].append(
                    f"{node.get('name', 'Unknown')} ({node.get('type', 'Unknown').split('.')[-1]})"
                )

        # ä¿®æ”¹çš„ç¯€é»
        for node_id in set(old_nodes.keys()) & set(new_nodes.keys()):
            old_node = old_nodes[node_id]
            new_node = new_nodes[node_id]
            if (old_node.get('name') != new_node.get('name') or
                old_node.get('type') != new_node.get('type') or
                old_node.get('parameters') != new_node.get('parameters')):
                changes['modified_nodes'].append(
                    f"{new_node.get('name', 'Unknown')} ({new_node.get('type', 'Unknown').split('.')[-1]})"
                )

        return changes

    def _format_change_summary(self, changes: Dict) -> str:
        """æ ¼å¼åŒ–è®Šæ›´æ‘˜è¦"""
        summary_parts = []

        for change_type, icon in [
            ('added_nodes', 'ğŸ†• æ–°å¢'),
            ('modified_nodes', 'âœï¸ ä¿®æ”¹'),
            ('removed_nodes', 'ğŸ—‘ï¸ åˆªé™¤')
        ]:
            nodes = changes[change_type]
            if nodes:
                preview = ', '.join(nodes[:3])
                if len(nodes) > 3:
                    preview += f" ç­‰ {len(nodes)} å€‹"
                summary_parts.append(f"{icon} {len(nodes)} å€‹ç¯€é»: {preview}")

        return '\n  '.join(summary_parts) if summary_parts else 'ç„¡æ˜é¡¯è®Šæ›´'

    # ========== æ•æ„Ÿè³‡è¨Šè™•ç† ==========

    def sanitize_workflow(self, workflow: Dict) -> Dict:
        """æ¸…ç†å·¥ä½œæµç¨‹ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        sanitized = copy.deepcopy(workflow)
        sensitive_keys = ['apiKey', 'api_key', 'password', 'token', 'secret', 'credential']

        if 'nodes' in sanitized:
            for node in sanitized['nodes']:
                if 'parameters' in node:
                    self._sanitize_dict(node['parameters'], sensitive_keys)

        return sanitized

    def _sanitize_dict(self, data: Dict, sensitive_keys: List[str]):
        """éè¿´æ··æ·†å­—å…¸ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        for key in list(data.keys()):
            if any(sensitive in key.lower() for sensitive in sensitive_keys):
                if isinstance(data[key], str) and len(data[key]) > 10:
                    data[key] = self._obfuscate_value(data[key])
            elif isinstance(data[key], str):
                obfuscated = self._obfuscate_value(data[key])
                if obfuscated != data[key]:
                    data[key] = obfuscated
            elif isinstance(data[key], dict):
                self._sanitize_dict(data[key], sensitive_keys)
            elif isinstance(data[key], list):
                for item in data[key]:
                    if isinstance(item, dict):
                        self._sanitize_dict(item, sensitive_keys)

    def _obfuscate_value(self, value: str) -> str:
        """æ··æ·†æ•æ„Ÿå€¼"""
        patterns = [
            (r'=?sk-ant-[a-zA-Z0-9\-_]+', lambda v: f"{v[:10]}{'*' * 20}{v[-4:]}" if len(v) > 14 else f"{v[:6]}{'*' * (len(v) - 6)}"),
            (r'sk-[a-zA-Z0-9]{48}', lambda v: f"{v[:8]}{'*' * 35}{v[-5:]}"),
            (r'eyJ[a-zA-Z0-9\-_]+\.[a-zA-Z0-9\-_]+', lambda v: f"{v.split('.')[0][:10]}...****...{v.split('.')[-1][-10:]}"),
            (r'gh[po]_[a-zA-Z0-9]{36}', lambda v: f"{v[:8]}{'*' * 25}{v[-5:]}")
        ]

        for pattern, obfuscator in patterns:
            if re.match(pattern, value):
                return obfuscator(value)

        return value

    def save_workflow(self, workflow: Dict) -> Path:
        """å„²å­˜å·¥ä½œæµç¨‹åˆ°æœ¬åœ°"""
        safe_name = "".join(c for c in workflow['name'] if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name[:100] if safe_name else "unnamed_workflow"

        filename = f"{workflow['id']}_{safe_name}.json"
        workflows_dir = self.git_repo_path / 'workflows'
        workflows_dir.mkdir(parents=True, exist_ok=True)

        filepath = workflows_dir / filename
        sanitized_workflow = self.sanitize_workflow(workflow)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(sanitized_workflow, f, indent=2, ensure_ascii=False)

        return filepath

    # ========== Git æ“ä½œ ==========

    def _run_git_command(self, cmd: List[str], check: bool = True) -> subprocess.CompletedProcess:
        """åŸ·è¡Œ Git å‘½ä»¤"""
        return subprocess.run(
            cmd,
            cwd=self.git_repo_path,
            check=check,
            capture_output=True,
            text=True,
            encoding='utf-8'
        )

    def git_commit_and_push(self, changed_workflows: List[str]) -> bool:
        """æäº¤è®Šæ›´åˆ° Git"""
        try:
            # Git add
            result = self._run_git_command(['git', 'add', '.'], check=False)
            if result.returncode != 0 and 'ignored by one of your .gitignore files' not in result.stderr:
                self.logger.error(f"âœ— Git add å¤±æ•—: {result.stderr}")
                return False

            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
            status = self._run_git_command(['git', 'status', '--porcelain'])
            if not status.stdout.strip():
                return True

            # Commit
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            commit_msg = f"[è‡ªå‹•å‚™ä»½] {timestamp}\n\nè®Šæ›´çš„å·¥ä½œæµç¨‹:\n" + "\n".join(f"- {name}" for name in changed_workflows)

            self._run_git_command(['git', 'commit', '-m', commit_msg])

            # Push (å«é‡è©¦æ©Ÿåˆ¶)
            for retry in range(3):
                try:
                    self._run_git_command(['git', 'push'])
                    self.logger.info(f"âœ“ æˆåŠŸæ¨é€ {len(changed_workflows)} å€‹å·¥ä½œæµç¨‹åˆ° Git")
                    return True

                except subprocess.CalledProcessError as e:
                    if 'no upstream branch' in e.stderr:
                        self._run_git_command(['git', 'push', '--set-upstream', 'origin', 'main'])
                        return True
                    elif 'rejected' in e.stderr or 'fetch first' in e.stderr:
                        # å…ˆ pull å†é‡è©¦
                        try:
                            self._run_git_command(['git', 'pull', '--no-rebase', '-X', 'theirs', 'origin', 'main'])
                            continue
                        except subprocess.CalledProcessError:
                            self.logger.warning("âš ï¸ Pull å¤±æ•—ï¼Œé‡ç½®åˆ°é ç«¯ç‹€æ…‹")
                            self._run_git_command(['git', 'fetch', 'origin', 'main'])
                            self._run_git_command(['git', 'reset', '--hard', 'origin/main'])
                            return False
                    elif retry < 2:
                        time.sleep(2 ** retry)
                    else:
                        raise

            return False

        except subprocess.CalledProcessError as e:
            self.logger.error(f"âœ— Git æ“ä½œå¤±æ•—: {e.cmd} (è¿”å›ç¢¼: {e.returncode})")
            return False
        except Exception as e:
            self.logger.error(f"âœ— Git æ“ä½œç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    # ========== å‚™ä»½æµç¨‹ ==========

    def backup_workflows(self) -> Dict:
        """åŸ·è¡Œå·¥ä½œæµç¨‹å‚™ä»½"""
        result = {
            'success': False,
            'changed_count': 0,
            'total_count': 0,
            'changed_workflows': [],
            'workflow_changes': {},
            'error': None
        }

        # å–å¾—æ‰€æœ‰å·¥ä½œæµç¨‹
        workflows = self.get_all_workflows()
        if workflows is None:
            result['error'] = 'ç„¡æ³•å–å¾—å·¥ä½œæµç¨‹åˆ—è¡¨'
            return result

        result['total_count'] = len(workflows)

        # è¼‰å…¥ä¸Šæ¬¡çš„ hash å’Œè³‡æ–™
        hash_file = self.git_repo_path / '.workflow_hashes.json'
        data_file = self.git_repo_path / '.workflow_data.json'

        old_hashes = json.load(open(hash_file, 'r', encoding='utf-8')) if hash_file.exists() else {}
        old_workflows = json.load(open(data_file, 'r', encoding='utf-8')) if data_file.exists() else {}

        new_hashes = {}
        new_workflows = {}
        changed_workflows = []

        # è™•ç†æ¯å€‹å·¥ä½œæµç¨‹
        for workflow in workflows:
            detail = self.get_workflow_detail(workflow['id'])
            if detail is None:
                continue

            current_hash = self.calculate_hash(detail)
            new_hashes[workflow['id']] = current_hash
            new_workflows[workflow['id']] = detail

            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
            if workflow['id'] not in old_hashes or old_hashes[workflow['id']] != current_hash:
                workflow_name = workflow['name']

                if workflow['id'] in old_workflows:
                    # åˆ†æè®Šæ›´
                    changes = self._analyze_workflow_changes(old_workflows[workflow['id']], detail)
                    has_real_changes = any(changes[k] for k in ['added_nodes', 'modified_nodes', 'removed_nodes'])

                    if has_real_changes:
                        change_summary = self._format_change_summary(changes)
                        self.logger.info(f"ğŸ“ {workflow_name}")
                        self.logger.info(f"   {change_summary}")
                        result['workflow_changes'][workflow_name] = change_summary
                        self.save_workflow(detail)
                        changed_workflows.append(workflow_name)
                else:
                    # æ–°å»ºç«‹çš„å·¥ä½œæµç¨‹
                    self.logger.info(f"ğŸ“ {workflow_name} (æ–°å»ºç«‹)")
                    result['workflow_changes'][workflow_name] = "ğŸ†• æ–°å»ºç«‹çš„å·¥ä½œæµç¨‹"
                    self.save_workflow(detail)
                    changed_workflows.append(workflow_name)

        # å„²å­˜æ–°çš„ hash å’Œè³‡æ–™
        with open(hash_file, 'w', encoding='utf-8') as f:
            json.dump(new_hashes, f, indent=2)

        sanitized_workflows = {wid: self.sanitize_workflow(wdata) for wid, wdata in new_workflows.items()}
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(sanitized_workflows, f, indent=2, ensure_ascii=False)

        result['changed_count'] = len(changed_workflows)
        result['changed_workflows'] = changed_workflows

        # æäº¤åˆ° Git
        if changed_workflows:
            if self.git_commit_and_push(changed_workflows):
                result['success'] = True
            else:
                result['error'] = 'Git æäº¤å¤±æ•—'
        else:
            result['success'] = True

        return result

    # ========== é€šçŸ¥ç³»çµ± ==========

    def send_webhook_notification(self, data: Dict):
        """ç™¼é€ Webhook é€šçŸ¥"""
        webhook_config = self.notifications.get('webhook', {})
        if not webhook_config.get('enabled', False):
            return

        try:
            platform = webhook_config.get('platform', 'generic')

            if platform == 'slack':
                payload = {
                    'text': data.get('message', ''),
                    'blocks': [{'type': 'section', 'text': {'type': 'mrkdwn', 'text': data.get('message', '')}}]
                }
            elif platform == 'discord':
                payload = {
                    'content': data.get('message', ''),
                    'embeds': [{
                        'title': data.get('title', 'n8n ç›£æ§é€šçŸ¥'),
                        'description': data.get('message', ''),
                        'color': 15158332 if data.get('status') == 'error' else 3066993
                    }]
                }
            elif platform == 'teams':
                payload = self._create_teams_card(data)
            else:
                payload = data

            response = requests.post(webhook_config['url'], json=payload, timeout=10)
            response.raise_for_status()

        except Exception as e:
            self.logger.error(f"âœ— Webhook ç™¼é€å¤±æ•—: {e}")

    def _create_teams_card(self, data: Dict) -> Dict:
        """å‰µå»º Microsoft Teams Adaptive Card"""
        status = data.get('status', 'info')
        title = data.get('title', 'n8n ç›£æ§é€šçŸ¥')

        color_map = {'error': 'Attention', 'success': 'Good', 'info': 'Default'}
        icon_map = {'error': 'âš ï¸', 'success': 'âœ…', 'info': 'â„¹ï¸'}

        card = {
            "type": "message",
            "attachments": [{
                "contentType": "application/vnd.microsoft.card.adaptive",
                "content": {
                    "type": "AdaptiveCard",
                    "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                    "version": "1.4",
                    "body": [{
                        "type": "TextBlock",
                        "text": f"{icon_map.get(status, 'â„¹ï¸')} {title}",
                        "size": "Large",
                        "weight": "Bolder",
                        "color": color_map.get(status, 'Default')
                    }]
                }
            }]
        }

        body = card["attachments"][0]["content"]["body"]

        # å‚™ä»½å®Œæˆé€šçŸ¥
        if 'backup_result' in data:
            result = data['backup_result']
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            body.append({
                "type": "FactSet",
                "facts": [
                    {"title": "â° å‚™ä»½æ™‚é–“", "value": timestamp},
                    {"title": "ğŸ“Š ç¸½æµç¨‹æ•¸", "value": str(result.get('total_count', 0))},
                    {"title": "âœï¸ æœ¬æ¬¡è®Šæ›´", "value": str(result.get('changed_count', 0))}
                ]
            })

            if result.get('changed_workflows'):
                body.append({
                    "type": "TextBlock",
                    "text": "**è®Šæ›´çš„å·¥ä½œæµç¨‹ï¼š**",
                    "weight": "Bolder",
                    "spacing": "Medium"
                })

                workflow_changes = result.get('workflow_changes', {})
                for workflow_name in result['changed_workflows']:
                    body.append({
                        "type": "TextBlock",
                        "text": f"ğŸ“ **{workflow_name}**",
                        "spacing": "Small",
                        "weight": "Bolder"
                    })

                    if workflow_name in workflow_changes:
                        for line in workflow_changes[workflow_name].split('\n'):
                            if line.strip():
                                body.append({
                                    "type": "TextBlock",
                                    "text": f"  {line.strip()}",
                                    "spacing": "None",
                                    "size": "Small",
                                    "isSubtle": True,
                                    "wrap": True
                                })

            card["attachments"][0]["content"]["actions"] = [
                {"type": "Action.OpenUrl", "title": "é–‹å•Ÿ n8n", "url": self.n8n_url},
                {"type": "Action.OpenUrl", "title": "æŸ¥çœ‹å‚™ä»½", "url": self.git_remote_url}
            ]

        # å¥åº·ç‹€æ…‹é€šçŸ¥
        elif 'health_status' in data:
            health = data['health_status']
            body.append({
                "type": "FactSet",
                "facts": [
                    {"title": "â° æ™‚é–“", "value": health.get('timestamp', '')},
                    {"title": "ğŸ“ ç‹€æ…‹", "value": health.get('status', 'unknown')}
                ]
            })

            if 'error' in health:
                body.append({
                    "type": "TextBlock",
                    "text": f"**éŒ¯èª¤è¨Šæ¯ï¼š**\n{health['error']}",
                    "wrap": True,
                    "spacing": "Medium",
                    "color": "Attention"
                })

            card["attachments"][0]["content"]["actions"] = [
                {"type": "Action.OpenUrl", "title": "æª¢æŸ¥ n8n", "url": self.n8n_url}
            ]

        # ä¸€èˆ¬è¨Šæ¯
        else:
            body.append({
                "type": "TextBlock",
                "text": data.get('message', ''),
                "wrap": True
            })

        return card

    # ========== ä¸»åŸ·è¡Œæµç¨‹ ==========

    def run(self):
        """åŸ·è¡Œå®Œæ•´çš„ç›£æ§èˆ‡å‚™ä»½æµç¨‹"""
        # å¥åº·æª¢æŸ¥
        health_status = self.check_health()
        self.handle_health_change(health_status)

        # åŸ·è¡Œå‚™ä»½
        if health_status['status'] == 'healthy':
            self.logger.info("ğŸ”„ é–‹å§‹å‚™ä»½å·¥ä½œæµç¨‹...")
            backup_result = self.backup_workflows()

            if backup_result['changed_count'] > 0:
                self.send_webhook_notification({
                    'title': 'n8nå·¥ä½œæµç¨‹ç•°å‹• - å‚™ä»½å®Œæˆ',
                    'status': 'success',
                    'backup_result': backup_result
                })
                self.logger.info(f"âœ“ å‚™ä»½å®Œæˆ ({backup_result['changed_count']}/{backup_result['total_count']} å€‹è®Šæ›´)")
            else:
                self.logger.info(f"âœ“ ç„¡è®Šæ›´ (å…± {backup_result['total_count']} å€‹å·¥ä½œæµç¨‹)")
        else:
            self.logger.warning("âš ï¸ æœå‹™ç•°å¸¸ï¼Œè·³éå‚™ä»½")

    def run_scheduled(self):
        """åŸ·è¡Œæ’ç¨‹æ¨¡å¼"""
        run_on_startup = self.schedule_config.get('run_on_startup', True)

        self.logger.info("=" * 50)
        self.logger.info("ğŸš€ n8n ç›£æ§ç³»çµ±å•Ÿå‹•")
        self.logger.info("â±ï¸  å¥åº·æª¢æŸ¥: æ¯ 10 åˆ†é˜ | å‚™ä»½: æ¯å°æ™‚")
        self.logger.info("=" * 50)

        # å•Ÿå‹•æ™‚åŸ·è¡Œ
        if run_on_startup:
            try:
                self.run()
            except KeyboardInterrupt:
                raise
            except Exception as e:
                self.logger.error(f"âœ— åŸ·è¡ŒéŒ¯èª¤: {e}")

        # æ’ç¨‹å¾ªç’°
        try:
            while True:
                now = datetime.now()

                # è¨ˆç®—ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“
                next_check = now.replace(second=0, microsecond=0)
                current_minute = now.minute
                next_minute = ((current_minute // 10) + 1) * 10

                if next_minute >= 60:
                    next_check = next_check.replace(minute=0) + timedelta(hours=1)
                else:
                    next_check = next_check.replace(minute=next_minute)

                is_backup_time = (next_minute == 0 or next_minute == 60)
                wait_seconds = (next_check - datetime.now()).total_seconds()

                task_type = "å¥åº·æª¢æŸ¥ + å‚™ä»½" if is_backup_time else "å¥åº·æª¢æŸ¥"
                self.logger.info(f"â° ä¸‹æ¬¡åŸ·è¡Œ: {next_check.strftime('%H:%M')} [{task_type}]")

                time.sleep(wait_seconds)

                try:
                    if is_backup_time:
                        self.run()
                    else:
                        health_status = self.check_health()
                        self.handle_health_change(health_status)
                except KeyboardInterrupt:
                    raise
                except Exception as e:
                    self.logger.error(f"âœ— åŸ·è¡ŒéŒ¯èª¤: {e}")

        except KeyboardInterrupt:
            self.logger.info("\n" + "=" * 50)
            self.logger.info("â›” ç›£æ§ç³»çµ±å·²åœæ­¢")
            self.logger.info("=" * 50)


if __name__ == '__main__':
    monitor = N8nMonitor('config.json')

    if monitor.schedule_config.get('enabled', False):
        monitor.run_scheduled()
    else:
        monitor.run()
