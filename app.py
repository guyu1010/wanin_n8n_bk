import requests
import json
import subprocess
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import logging
import time

class N8nMonitor:
    def __init__(self, config_path: str = 'config.json'):
        """åˆå§‹åŒ–ç›£æ§ç³»çµ±"""
        self.load_config(config_path)
        self.setup_logging()
        self.last_health_status = None
        
    def load_config(self, config_path: str):
        """è¼‰å…¥è¨­å®šæª”"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self.n8n_url = config['n8n']['url'].rstrip('/')
        self.api_key = config['n8n']['api_key']
        self.git_repo_path = Path(config['git']['repo_path'])
        
        # é€šçŸ¥è¨­å®š
        self.notifications = config.get('notifications', {})
        
        # HTTP è«‹æ±‚è¨­å®š
        self.headers = {
            'X-N8N-API-KEY': self.api_key,
            'Accept': 'application/json'
        }
        self.timeout = config.get('timeout', 10)
        self.max_retries = config.get('max_retries', 3)
    
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('n8n_monitor.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def check_health(self) -> Dict:
        """æª¢æŸ¥ n8n å¥åº·ç‹€æ…‹"""
        try:
            # æ–¹æ³• 1: ä½¿ç”¨ n8n çš„ healthz endpoint
            health_url = f"{self.n8n_url}/healthz"
            response = requests.get(health_url, timeout=self.timeout)
            
            if response.status_code == 200:
                self.logger.info("âœ“ n8n æœå‹™æ­£å¸¸é‹è¡Œ")
                return {
                    'status': 'healthy',
                    'response_time': response.elapsed.total_seconds(),
                    'timestamp': datetime.now().isoformat()
                }
            else:
                self.logger.warning(f"n8n å›æ‡‰ç•°å¸¸: HTTP {response.status_code}")
                return {
                    'status': 'unhealthy',
                    'error': f'HTTP {response.status_code}',
                    'timestamp': datetime.now().isoformat()
                }
                
        except requests.exceptions.Timeout:
            self.logger.error("âœ— n8n é€£ç·šé€¾æ™‚")
            return {
                'status': 'timeout',
                'error': 'Connection timeout',
                'timestamp': datetime.now().isoformat()
            }
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"âœ— ç„¡æ³•é€£ç·šåˆ° n8n: {e}")
            return {
                'status': 'down',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            self.logger.error(f"âœ— å¥åº·æª¢æŸ¥ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def get_all_workflows(self) -> Optional[List[Dict]]:
        """å–å¾—æ‰€æœ‰å·¥ä½œæµç¨‹(å¸¶é‡è©¦æ©Ÿåˆ¶)"""
        url = f"{self.n8n_url}/api/v1/workflows"
        
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                workflows = response.json()['data']
                self.logger.info(f"æˆåŠŸå–å¾— {len(workflows)} å€‹å·¥ä½œæµç¨‹")
                return workflows
            except Exception as e:
                self.logger.warning(f"å–å¾—å·¥ä½œæµç¨‹å¤±æ•— (å˜—è©¦ {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                else:
                    self.logger.error("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    return None
    
    def get_workflow_detail(self, workflow_id: str) -> Optional[Dict]:
        """å–å¾—å·¥ä½œæµç¨‹è©³ç´°å…§å®¹"""
        url = f"{self.n8n_url}/api/v1/workflows/{workflow_id}"
        try:
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            self.logger.error(f"å–å¾—å·¥ä½œæµç¨‹ {workflow_id} å¤±æ•—: {e}")
            return None
    
    def calculate_hash(self, workflow_data: Dict) -> str:
        """è¨ˆç®—å·¥ä½œæµç¨‹çš„ hash å€¼"""
        # ç§»é™¤æ™‚é–“æˆ³è¨˜ç­‰ä¸å½±éŸ¿é‚è¼¯çš„æ¬„ä½
        clean_data = {k: v for k, v in workflow_data.items() 
                     if k not in ['updatedAt', 'createdAt']}
        content = json.dumps(clean_data, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def sanitize_workflow(self, workflow: Dict) -> Dict:
        """æ¸…ç†å·¥ä½œæµç¨‹ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        import copy
        sanitized = copy.deepcopy(workflow)

        # æ¸…ç†ç¯€é»ä¸­çš„æ•æ„Ÿåƒæ•¸
        sensitive_keys = ['apiKey', 'api_key', 'password', 'token', 'secret', 'credential']

        if 'nodes' in sanitized:
            for node in sanitized['nodes']:
                if 'parameters' in node:
                    for key in list(node['parameters'].keys()):
                        # æª¢æŸ¥æ˜¯å¦ç‚ºæ•æ„Ÿæ¬„ä½
                        if any(sensitive in key.lower() for sensitive in sensitive_keys):
                            node['parameters'][key] = "***REMOVED***"
                        # éè¿´æ¸…ç†å·¢ç‹€çµæ§‹
                        elif isinstance(node['parameters'][key], dict):
                            self._sanitize_dict(node['parameters'][key], sensitive_keys)

        return sanitized

    def _obfuscate_value(self, value: str) -> str:
        """ç°¡å–®æ··æ·†æ•æ„Ÿå€¼ï¼ˆä¿ç•™å‰å¾Œéƒ¨åˆ†ï¼Œä¸­é–“ç”¨ * ä»£æ›¿ï¼‰"""
        import re

        # Anthropic API keys: sk-ant-xxx
        if re.match(r'=?sk-ant-[a-zA-Z0-9\-_]+', value):
            # ä¿ç•™å‰10å€‹å­—å…ƒå’Œå¾Œ4å€‹å­—å…ƒ
            if len(value) > 14:
                prefix = value[:10]
                suffix = value[-4:]
                return f"{prefix}{'*' * 20}{suffix}"
            return value[:6] + '*' * (len(value) - 6)

        # OpenAI API keys: sk-xxx
        if re.match(r'sk-[a-zA-Z0-9]{48}', value):
            return value[:8] + '*' * 35 + value[-5:]

        # JWT tokens
        if re.match(r'eyJ[a-zA-Z0-9\-_]+\.[a-zA-Z0-9\-_]+', value):
            parts = value.split('.')
            if len(parts) >= 2:
                return f"{parts[0][:10]}...****...{parts[-1][-10:]}"

        # GitHub tokens
        if re.match(r'gh[po]_[a-zA-Z0-9]{36}', value):
            return value[:8] + '*' * 25 + value[-5:]

        return value

    def _sanitize_dict(self, data: Dict, sensitive_keys: List[str]):
        """éè¿´æ··æ·†å­—å…¸ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        for key in list(data.keys()):
            if any(sensitive in key.lower() for sensitive in sensitive_keys):
                # åªæ··æ·†å­—ä¸²å€¼
                if isinstance(data[key], str) and len(data[key]) > 10:
                    data[key] = self._obfuscate_value(data[key])
            elif isinstance(data[key], str):
                # æª¢æŸ¥å­—ä¸²å€¼æ˜¯å¦åŒ…å«æ•æ„Ÿæ¨¡å¼
                obfuscated = self._obfuscate_value(data[key])
                if obfuscated != data[key]:
                    data[key] = obfuscated
            elif isinstance(data[key], dict):
                self._sanitize_dict(data[key], sensitive_keys)
            elif isinstance(data[key], list):
                for item in data[key]:
                    if isinstance(item, dict):
                        self._sanitize_dict(item, sensitive_keys)

    def save_workflow(self, workflow: Dict) -> Path:
        """å„²å­˜å·¥ä½œæµç¨‹åˆ°æœ¬åœ°"""
        # æ¸…ç†æª”åä¸­çš„ç‰¹æ®Šå­—å…ƒ
        safe_name = "".join(c for c in workflow['name'] if c.isalnum() or c in (' ', '-', '_')).strip()

        # å¦‚æœæ¸…ç†å¾Œæ˜¯ç©ºå­—ä¸²ï¼Œä½¿ç”¨é è¨­åç¨±
        if not safe_name:
            safe_name = "unnamed_workflow"
            self.logger.warning(f"å·¥ä½œæµç¨‹ ID {workflow['id']} çš„åç¨±ç„¡æ³•æ¸…ç†ï¼Œä½¿ç”¨é è¨­åç¨±")

        # é™åˆ¶æª”åé•·åº¦ï¼Œé¿å…éé•·
        if len(safe_name) > 100:
            safe_name = safe_name[:100]
            self.logger.info(f"æª”åéé•·ï¼Œå·²æˆªæ–·è‡³ 100 å­—å…ƒ")

        filename = f"{workflow['id']}_{safe_name}.json"

        workflows_dir = self.git_repo_path / 'workflows'
        workflows_dir.mkdir(parents=True, exist_ok=True)

        filepath = workflows_dir / filename

        # ç°¡å–®æ··æ·†æ•æ„Ÿè³‡è¨Šå¾Œå„²å­˜
        sanitized_workflow = self.sanitize_workflow(workflow)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(sanitized_workflow, f, indent=2, ensure_ascii=False)

        return filepath
    
    def git_commit_and_push(self, changed_workflows: List[str]) -> bool:
        """æäº¤è®Šæ›´åˆ° Git"""
        try:
            self.logger.info("=" * 50)
            self.logger.info("é–‹å§‹åŸ·è¡Œ Git æ“ä½œ")

            # ç¢ºä¿åœ¨ git repo ç›®éŒ„ï¼ŒåŸ·è¡Œ git add
            self.logger.info(f"åŸ·è¡Œ: git add . (åœ¨ç›®éŒ„: {self.git_repo_path})")
            result = subprocess.run(['git', 'add', '.'],
                         cwd=self.git_repo_path, check=True,
                         capture_output=True, text=True)

            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
            self.logger.info("æª¢æŸ¥ git status...")
            status = subprocess.run(['git', 'status', '--porcelain'],
                                  cwd=self.git_repo_path, check=True,
                                  capture_output=True, text=True)

            if not status.stdout.strip():
                self.logger.info("æ²’æœ‰éœ€è¦æäº¤çš„è®Šæ›´")
                return True

            self.logger.info(f"åµæ¸¬åˆ°è®Šæ›´æª”æ¡ˆ:\n{status.stdout}")

            # å»ºç«‹ commit message
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            commit_msg = f"[è‡ªå‹•å‚™ä»½] {timestamp}\n\nè®Šæ›´çš„å·¥ä½œæµç¨‹:\n"
            commit_msg += "\n".join(f"- {name}" for name in changed_workflows)

            # åŸ·è¡Œ commit
            self.logger.info("åŸ·è¡Œ: git commit")
            subprocess.run(['git', 'commit', '-m', commit_msg],
                         cwd=self.git_repo_path, check=True,
                         capture_output=True, text=True)
            self.logger.info("âœ“ Commit æˆåŠŸ")

            # åŸ·è¡Œ pushï¼ˆé¦–æ¬¡éœ€è¦è¨­å®š upstreamï¼‰
            self.logger.info("åŸ·è¡Œ: git push")
            try:
                push_result = subprocess.run(['git', 'push'],
                             cwd=self.git_repo_path, check=True,
                             capture_output=True, text=True)
            except subprocess.CalledProcessError as e:
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ pushï¼Œéœ€è¦è¨­å®š upstream
                if 'no upstream branch' in e.stderr:
                    self.logger.info("åµæ¸¬åˆ°é¦–æ¬¡æ¨é€ï¼ŒåŸ·è¡Œ: git push --set-upstream origin main")
                    push_result = subprocess.run(['git', 'push', '--set-upstream', 'origin', 'main'],
                                 cwd=self.git_repo_path, check=True,
                                 capture_output=True, text=True)
                else:
                    raise

            if push_result.stdout:
                self.logger.info(f"Push è¼¸å‡º: {push_result.stdout}")
            if push_result.stderr:
                self.logger.info(f"Push è¨Šæ¯: {push_result.stderr}")

            self.logger.info(f"âœ“ æˆåŠŸæäº¤ä¸¦æ¨é€ {len(changed_workflows)} å€‹å·¥ä½œæµç¨‹åˆ° Git")
            self.logger.info("=" * 50)
            return True

        except subprocess.CalledProcessError as e:
            self.logger.error("=" * 50)
            self.logger.error(f"âœ— Git æ“ä½œå¤±æ•—")
            self.logger.error(f"æŒ‡ä»¤: {e.cmd}")
            self.logger.error(f"è¿”å›ç¢¼: {e.returncode}")
            if e.stdout:
                self.logger.error(f"æ¨™æº–è¼¸å‡º: {e.stdout}")
            if e.stderr:
                self.logger.error(f"éŒ¯èª¤è¼¸å‡º: {e.stderr}")
            self.logger.error("=" * 50)
            return False
        except Exception as e:
            self.logger.error(f"âœ— Git æ“ä½œç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            return False
    
    def backup_workflows(self) -> Dict:
        """åŸ·è¡Œå·¥ä½œæµç¨‹å‚™ä»½"""
        self.logger.info("=" * 50)
        self.logger.info("é–‹å§‹å‚™ä»½å·¥ä½œæµç¨‹")
        
        result = {
            'success': False,
            'changed_count': 0,
            'total_count': 0,
            'changed_workflows': [],
            'error': None
        }
        
        # å–å¾—æ‰€æœ‰å·¥ä½œæµç¨‹
        workflows = self.get_all_workflows()
        if workflows is None:
            result['error'] = 'ç„¡æ³•å–å¾—å·¥ä½œæµç¨‹åˆ—è¡¨'
            return result
        
        result['total_count'] = len(workflows)
        
        # è¼‰å…¥ä¸Šæ¬¡çš„ hash ç´€éŒ„
        hash_file = self.git_repo_path / '.workflow_hashes.json'
        old_hashes = {}
        if hash_file.exists():
            with open(hash_file, 'r', encoding='utf-8') as f:
                old_hashes = json.load(f)
        
        new_hashes = {}
        changed_workflows = []
        
        # è™•ç†æ¯å€‹å·¥ä½œæµç¨‹
        for workflow in workflows:
            detail = self.get_workflow_detail(workflow['id'])
            if detail is None:
                continue
            
            # è¨ˆç®— hash
            current_hash = self.calculate_hash(detail)
            new_hashes[workflow['id']] = current_hash
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
            if workflow['id'] not in old_hashes or old_hashes[workflow['id']] != current_hash:
                self.logger.info(f"åµæ¸¬åˆ°è®Šæ›´: {workflow['name']} (ID: {workflow['id']})")
                self.save_workflow(detail)
                changed_workflows.append(workflow['name'])
        
        # å„²å­˜æ–°çš„ hash ç´€éŒ„
        with open(hash_file, 'w', encoding='utf-8') as f:
            json.dump(new_hashes, f, indent=2)
        
        result['changed_count'] = len(changed_workflows)
        result['changed_workflows'] = changed_workflows
        
        # å¦‚æœæœ‰è®Šæ›´,æäº¤åˆ° Git
        if changed_workflows:
            if self.git_commit_and_push(changed_workflows):
                result['success'] = True
            else:
                result['error'] = 'Git æäº¤å¤±æ•—'
        else:
            self.logger.info("æ²’æœ‰åµæ¸¬åˆ°å·¥ä½œæµç¨‹è®Šæ›´")
            result['success'] = True
        
        return result
    
    def send_webhook_notification(self, data: Dict):
        """ç™¼é€ Webhook é€šçŸ¥ (Slack, Discord, Teams, etc.)"""
        webhook_config = self.notifications.get('webhook', {})
        if not webhook_config.get('enabled', False):
            return

        try:
            # æ ¹æ“šä¸åŒå¹³å°æ ¼å¼åŒ–è¨Šæ¯
            platform = webhook_config.get('platform', 'generic')

            if platform == 'slack':
                payload = {
                    'text': data.get('message', ''),
                    'blocks': [
                        {
                            'type': 'section',
                            'text': {'type': 'mrkdwn', 'text': data.get('message', '')}
                        }
                    ]
                }
            elif platform == 'discord':
                payload = {
                    'content': data.get('message', ''),
                    'embeds': [{
                        'title': data.get('title', 'n8n ç›£æ§é€šçŸ¥'),
                        'description': data.get('message', ''),
                        'color': 15158332 if data.get('status') == 'error' else 3066993
                    }]
                }
            elif platform == 'teams':
                # Power Automate ä½¿ç”¨ç°¡å–®çš„ JSONï¼Œåœ¨ Flow ä¸­å»ºç«‹å¡ç‰‡
                payload = self._create_teams_payload(data)
            else:  # generic
                payload = data

            response = requests.post(
                webhook_config['url'],
                json=payload,
                timeout=10
            )
            response.raise_for_status()
            self.logger.info("âœ“ Webhook é€šçŸ¥å·²ç™¼é€")

        except Exception as e:
            self.logger.error(f"ç™¼é€ Webhook å¤±æ•—: {e}")

    def _create_teams_payload(self, data: Dict) -> Dict:
        """å‰µå»ºçµ¦ Power Automate çš„ç°¡å–® JSON payload"""
        status = data.get('status', 'info')
        title = data.get('title', 'n8n ç›£æ§é€šçŸ¥')

        # åŸºæœ¬ payload
        payload = {
            'title': title,
            'status': status,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'n8n_url': self.n8n_url
        }

        # æ ¹æ“šä¸åŒé¡å‹æ·»åŠ è³‡æ–™
        if 'backup_result' in data:
            result = data['backup_result']
            payload.update({
                'type': 'backup',
                'total_count': result.get('total_count', 0),
                'changed_count': result.get('changed_count', 0),
                'changed_workflows': result.get('changed_workflows', []),
                'github_url': 'https://github.com/guyu1010/wanin_n8n_bk_data'
            })
        elif 'health_status' in data:
            health = data['health_status']
            payload.update({
                'type': 'health',
                'health_status': health.get('status', 'unknown'),
                'error': health.get('error', '')
            })

        return payload

    def _create_teams_card(self, data: Dict) -> Dict:
        """å‰µå»º Microsoft Teams Adaptive Card"""
        status = data.get('status', 'info')
        title = data.get('title', 'n8n ç›£æ§é€šçŸ¥')

        # æ ¹æ“šç‹€æ…‹è¨­å®šé¡è‰²å’Œåœ–ç¤º
        if status == 'error':
            color = 'Attention'  # ç´…è‰²
            icon = 'âš ï¸'
        elif status == 'success':
            color = 'Good'  # ç¶ è‰²
            icon = 'âœ…'
        else:
            color = 'Default'  # ç°è‰²
            icon = 'â„¹ï¸'

        # åŸºæœ¬å¡ç‰‡çµæ§‹
        card = {
            "type": "message",
            "attachments": [
                {
                    "contentType": "application/vnd.microsoft.card.adaptive",
                    "content": {
                        "type": "AdaptiveCard",
                        "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                        "version": "1.4",
                        "body": [
                            {
                                "type": "TextBlock",
                                "text": f"{icon} {title}",
                                "size": "Large",
                                "weight": "Bolder",
                                "color": color
                            }
                        ]
                    }
                }
            ]
        }

        body = card["attachments"][0]["content"]["body"]

        # æ ¹æ“šä¸åŒçš„é€šçŸ¥é¡å‹æ·»åŠ å…§å®¹
        if 'backup_result' in data:
            # å‚™ä»½å®Œæˆé€šçŸ¥
            result = data['backup_result']
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # æ·»åŠ æ™‚é–“å’Œçµ±è¨ˆè³‡è¨Š
            body.append({
                "type": "FactSet",
                "facts": [
                    {"title": "â° å‚™ä»½æ™‚é–“", "value": timestamp},
                    {"title": "ğŸ“Š ç¸½æµç¨‹æ•¸", "value": str(result.get('total_count', 0))},
                    {"title": "âœï¸ æœ¬æ¬¡è®Šæ›´", "value": str(result.get('changed_count', 0))}
                ]
            })

            # å¦‚æœæœ‰è®Šæ›´ï¼Œé¡¯ç¤ºè®Šæ›´åˆ—è¡¨
            if result.get('changed_workflows'):
                body.append({
                    "type": "TextBlock",
                    "text": "**è®Šæ›´çš„å·¥ä½œæµç¨‹ï¼š**",
                    "weight": "Bolder",
                    "spacing": "Medium"
                })

                for workflow_name in result['changed_workflows']:
                    body.append({
                        "type": "TextBlock",
                        "text": f"â€¢ {workflow_name}",
                        "spacing": "Small"
                    })

            # æ·»åŠ é€£çµæŒ‰éˆ•
            card["attachments"][0]["content"]["actions"] = [
                {
                    "type": "Action.OpenUrl",
                    "title": "é–‹å•Ÿ n8n",
                    "url": self.n8n_url
                },
                {
                    "type": "Action.OpenUrl",
                    "title": "æŸ¥çœ‹å‚™ä»½",
                    "url": "https://github.com/guyu1010/wanin_n8n_bk_data"
                }
            ]

        elif 'health_status' in data:
            # å¥åº·ç‹€æ…‹è®Šæ›´é€šçŸ¥
            health = data['health_status']

            body.append({
                "type": "FactSet",
                "facts": [
                    {"title": "â° æ™‚é–“", "value": health.get('timestamp', '')},
                    {"title": "ğŸ“ ç‹€æ…‹", "value": health.get('status', 'unknown')}
                ]
            })

            # å¦‚æœæœ‰éŒ¯èª¤è¨Šæ¯
            if 'error' in health:
                body.append({
                    "type": "TextBlock",
                    "text": f"**éŒ¯èª¤è¨Šæ¯ï¼š**\n{health['error']}",
                    "wrap": True,
                    "spacing": "Medium",
                    "color": "Attention"
                })

            # æ·»åŠ é€£çµæŒ‰éˆ•
            card["attachments"][0]["content"]["actions"] = [
                {
                    "type": "Action.OpenUrl",
                    "title": "æª¢æŸ¥ n8n",
                    "url": self.n8n_url
                }
            ]
        else:
            # ä¸€èˆ¬è¨Šæ¯
            message = data.get('message', '')
            body.append({
                "type": "TextBlock",
                "text": message,
                "wrap": True
            })

        return card

    def handle_health_change(self, health_status: Dict):
        """è™•ç†å¥åº·ç‹€æ…‹è®Šæ›´"""
        current_status = health_status['status']

        # å¦‚æœç‹€æ…‹æ”¹è®Š,ç™¼é€é€šçŸ¥
        if self.last_health_status != current_status:
            if current_status != 'healthy':
                # n8n å‡ºç¾å•é¡Œ
                self.logger.error(f"n8n æœå‹™ç•°å¸¸: {current_status}")
                self.send_webhook_notification({
                    'title': 'n8n æœå‹™ç•°å¸¸',
                    'status': 'error',
                    'health_status': health_status
                })
            else:
                # n8n æ¢å¾©æ­£å¸¸
                self.logger.info("n8n æœå‹™å·²æ¢å¾©æ­£å¸¸")
                self.send_webhook_notification({
                    'title': 'n8n æœå‹™æ¢å¾©',
                    'status': 'success',
                    'health_status': health_status
                })

            self.last_health_status = current_status
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´çš„ç›£æ§èˆ‡å‚™ä»½æµç¨‹"""
        self.logger.info("é–‹å§‹åŸ·è¡Œ n8n ç›£æ§èˆ‡å‚™ä»½")
        
        # 1. æª¢æŸ¥å¥åº·ç‹€æ…‹
        health_status = self.check_health()
        self.handle_health_change(health_status)
        
        # 2. å¦‚æœ n8n æ­£å¸¸,åŸ·è¡Œå‚™ä»½
        if health_status['status'] == 'healthy':
            backup_result = self.backup_workflows()

            if backup_result['changed_count'] > 0:
                self.send_webhook_notification({
                    'title': 'n8n å·¥ä½œæµç¨‹å‚™ä»½å®Œæˆ',
                    'status': 'success',
                    'backup_result': backup_result
                })
        else:
            self.logger.warning("ç”±æ–¼ n8n æœå‹™ç•°å¸¸,è·³éå‚™ä»½ä½œæ¥­")
        
        self.logger.info("ç›£æ§èˆ‡å‚™ä»½æµç¨‹çµæŸ")
        self.logger.info("=" * 50)

if __name__ == '__main__':
    monitor = N8nMonitor('config.json')
    monitor.run()