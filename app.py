import requests
import json
import subprocess
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import logging
import time

class N8nMonitor:
    def __init__(self, config_path: str = 'config.json'):
        """åˆå§‹åŒ–ç›£æ§ç³»çµ±"""
        self.load_config(config_path)
        self.setup_logging()
        self.last_health_status = None
        
    def load_config(self, config_path: str):
        """è¼‰å…¥è¨­å®šæª”"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self.n8n_url = config['n8n']['url'].rstrip('/')
        self.api_key = config['n8n']['api_key']
        self.git_repo_path = Path(config['git']['repo_path'])

        # é€šçŸ¥è¨­å®š
        self.notifications = config.get('notifications', {})

        # æ’ç¨‹è¨­å®š
        self.schedule_config = config.get('schedule', {
            'enabled': False,
            'interval': 600,
            'run_on_startup': True
        })

        # HTTP è«‹æ±‚è¨­å®š
        self.headers = {
            'X-N8N-API-KEY': self.api_key,
            'Accept': 'application/json'
        }
        self.timeout = config.get('timeout', 10)
        self.max_retries = config.get('max_retries', 3)
    
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('n8n_monitor.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def check_health(self) -> Dict:
        """æª¢æŸ¥ n8n å¥åº·ç‹€æ…‹"""
        try:
            # æ–¹æ³• 1: ä½¿ç”¨ n8n çš„ healthz endpoint
            health_url = f"{self.n8n_url}/healthz"
            response = requests.get(health_url, timeout=self.timeout)
            
            if response.status_code == 200:
                self.logger.info("âœ“ n8n æœå‹™æ­£å¸¸é‹è¡Œ")
                return {
                    'status': 'healthy',
                    'response_time': response.elapsed.total_seconds(),
                    'timestamp': datetime.now().isoformat()
                }
            else:
                self.logger.warning(f"n8n å›æ‡‰ç•°å¸¸: HTTP {response.status_code}")
                return {
                    'status': 'unhealthy',
                    'error': f'HTTP {response.status_code}',
                    'timestamp': datetime.now().isoformat()
                }
                
        except requests.exceptions.Timeout:
            self.logger.error("âœ— n8n é€£ç·šé€¾æ™‚")
            return {
                'status': 'timeout',
                'error': 'Connection timeout',
                'timestamp': datetime.now().isoformat()
            }
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"âœ— ç„¡æ³•é€£ç·šåˆ° n8n: {e}")
            return {
                'status': 'down',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            self.logger.error(f"âœ— å¥åº·æª¢æŸ¥ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def get_all_workflows(self) -> Optional[List[Dict]]:
        """å–å¾—æ‰€æœ‰å·¥ä½œæµç¨‹(å¸¶é‡è©¦æ©Ÿåˆ¶)"""
        url = f"{self.n8n_url}/api/v1/workflows"
        
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                workflows = response.json()['data']
                self.logger.info(f"æˆåŠŸå–å¾— {len(workflows)} å€‹å·¥ä½œæµç¨‹")
                return workflows
            except Exception as e:
                self.logger.warning(f"å–å¾—å·¥ä½œæµç¨‹å¤±æ•— (å˜—è©¦ {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                else:
                    self.logger.error("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    return None
    
    def get_workflow_detail(self, workflow_id: str) -> Optional[Dict]:
        """å–å¾—å·¥ä½œæµç¨‹è©³ç´°å…§å®¹"""
        url = f"{self.n8n_url}/api/v1/workflows/{workflow_id}"
        try:
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            self.logger.error(f"å–å¾—å·¥ä½œæµç¨‹ {workflow_id} å¤±æ•—: {e}")
            return None
    
    def calculate_hash(self, workflow_data: Dict) -> str:
        """è¨ˆç®—å·¥ä½œæµç¨‹çš„ hash å€¼"""
        # ç§»é™¤æ™‚é–“æˆ³è¨˜ç­‰ä¸å½±éŸ¿é‚è¼¯çš„æ¬„ä½
        clean_data = {k: v for k, v in workflow_data.items()
                     if k not in ['updatedAt', 'createdAt']}
        content = json.dumps(clean_data, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

    def _analyze_workflow_changes(self, old_workflow: Dict, new_workflow: Dict) -> Dict:
        """åˆ†æå·¥ä½œæµç¨‹çš„è®Šæ›´"""
        changes = {
            'added_nodes': [],
            'removed_nodes': [],
            'modified_nodes': []
        }

        old_nodes = {node['id']: node for node in old_workflow.get('nodes', [])}
        new_nodes = {node['id']: node for node in new_workflow.get('nodes', [])}

        for node_id, node in new_nodes.items():
            if node_id not in old_nodes:
                changes['added_nodes'].append(f"{node.get('name', 'Unknown')} ({node.get('type', 'Unknown').split('.')[-1]})")

        for node_id, node in old_nodes.items():
            if node_id not in new_nodes:
                changes['removed_nodes'].append(f"{node.get('name', 'Unknown')} ({node.get('type', 'Unknown').split('.')[-1]})")

        for node_id in set(old_nodes.keys()) & set(new_nodes.keys()):
            old_node = old_nodes[node_id]
            new_node = new_nodes[node_id]
            if (old_node.get('name') != new_node.get('name') or
                old_node.get('type') != new_node.get('type') or
                old_node.get('parameters') != new_node.get('parameters')):
                changes['modified_nodes'].append(f"{new_node.get('name', 'Unknown')} ({new_node.get('type', 'Unknown').split('.')[-1]})")

        return changes

    def _format_change_summary(self, changes: Dict) -> str:
        """æ ¼å¼åŒ–è®Šæ›´æ‘˜è¦ç‚ºç°¡æ½”æ–‡å­—"""
        summary_parts = []

        if changes['added_nodes']:
            summary_parts.append(f"ğŸ†• æ–°å¢ {len(changes['added_nodes'])} å€‹ç¯€é»: {', '.join(changes['added_nodes'][:3])}")
            if len(changes['added_nodes']) > 3:
                summary_parts[-1] += f" ç­‰ {len(changes['added_nodes'])} å€‹"

        if changes['modified_nodes']:
            summary_parts.append(f"âœï¸ ä¿®æ”¹ {len(changes['modified_nodes'])} å€‹ç¯€é»: {', '.join(changes['modified_nodes'][:3])}")
            if len(changes['modified_nodes']) > 3:
                summary_parts[-1] += f" ç­‰ {len(changes['modified_nodes'])} å€‹"

        if changes['removed_nodes']:
            summary_parts.append(f"ğŸ—‘ï¸ åˆªé™¤ {len(changes['removed_nodes'])} å€‹ç¯€é»: {', '.join(changes['removed_nodes'][:3])}")
            if len(changes['removed_nodes']) > 3:
                summary_parts[-1] += f" ç­‰ {len(changes['removed_nodes'])} å€‹"

        return '\n  '.join(summary_parts) if summary_parts else 'ç„¡æ˜é¡¯è®Šæ›´'

    def sanitize_workflow(self, workflow: Dict) -> Dict:
        """æ¸…ç†å·¥ä½œæµç¨‹ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        import copy
        sanitized = copy.deepcopy(workflow)

        # æ¸…ç†ç¯€é»ä¸­çš„æ•æ„Ÿåƒæ•¸
        sensitive_keys = ['apiKey', 'api_key', 'password', 'token', 'secret', 'credential']

        if 'nodes' in sanitized:
            for node in sanitized['nodes']:
                if 'parameters' in node:
                    for key in list(node['parameters'].keys()):
                        # æª¢æŸ¥æ˜¯å¦ç‚ºæ•æ„Ÿæ¬„ä½
                        if any(sensitive in key.lower() for sensitive in sensitive_keys):
                            node['parameters'][key] = "***REMOVED***"
                        # éè¿´æ¸…ç†å·¢ç‹€çµæ§‹
                        elif isinstance(node['parameters'][key], dict):
                            self._sanitize_dict(node['parameters'][key], sensitive_keys)

        return sanitized

    def _obfuscate_value(self, value: str) -> str:
        """ç°¡å–®æ··æ·†æ•æ„Ÿå€¼ï¼ˆä¿ç•™å‰å¾Œéƒ¨åˆ†ï¼Œä¸­é–“ç”¨ * ä»£æ›¿ï¼‰"""
        import re

        # Anthropic API keys: sk-ant-xxx
        if re.match(r'=?sk-ant-[a-zA-Z0-9\-_]+', value):
            # ä¿ç•™å‰10å€‹å­—å…ƒå’Œå¾Œ4å€‹å­—å…ƒ
            if len(value) > 14:
                prefix = value[:10]
                suffix = value[-4:]
                return f"{prefix}{'*' * 20}{suffix}"
            return value[:6] + '*' * (len(value) - 6)

        # OpenAI API keys: sk-xxx
        if re.match(r'sk-[a-zA-Z0-9]{48}', value):
            return value[:8] + '*' * 35 + value[-5:]

        # JWT tokens
        if re.match(r'eyJ[a-zA-Z0-9\-_]+\.[a-zA-Z0-9\-_]+', value):
            parts = value.split('.')
            if len(parts) >= 2:
                return f"{parts[0][:10]}...****...{parts[-1][-10:]}"

        # GitHub tokens
        if re.match(r'gh[po]_[a-zA-Z0-9]{36}', value):
            return value[:8] + '*' * 25 + value[-5:]

        return value

    def _sanitize_dict(self, data: Dict, sensitive_keys: List[str]):
        """éè¿´æ··æ·†å­—å…¸ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        for key in list(data.keys()):
            if any(sensitive in key.lower() for sensitive in sensitive_keys):
                # åªæ··æ·†å­—ä¸²å€¼
                if isinstance(data[key], str) and len(data[key]) > 10:
                    data[key] = self._obfuscate_value(data[key])
            elif isinstance(data[key], str):
                # æª¢æŸ¥å­—ä¸²å€¼æ˜¯å¦åŒ…å«æ•æ„Ÿæ¨¡å¼
                obfuscated = self._obfuscate_value(data[key])
                if obfuscated != data[key]:
                    data[key] = obfuscated
            elif isinstance(data[key], dict):
                self._sanitize_dict(data[key], sensitive_keys)
            elif isinstance(data[key], list):
                for item in data[key]:
                    if isinstance(item, dict):
                        self._sanitize_dict(item, sensitive_keys)

    def save_workflow(self, workflow: Dict) -> Path:
        """å„²å­˜å·¥ä½œæµç¨‹åˆ°æœ¬åœ°"""
        # æ¸…ç†æª”åä¸­çš„ç‰¹æ®Šå­—å…ƒ
        safe_name = "".join(c for c in workflow['name'] if c.isalnum() or c in (' ', '-', '_')).strip()

        # å¦‚æœæ¸…ç†å¾Œæ˜¯ç©ºå­—ä¸²ï¼Œä½¿ç”¨é è¨­åç¨±
        if not safe_name:
            safe_name = "unnamed_workflow"
            self.logger.warning(f"å·¥ä½œæµç¨‹ ID {workflow['id']} çš„åç¨±ç„¡æ³•æ¸…ç†ï¼Œä½¿ç”¨é è¨­åç¨±")

        # é™åˆ¶æª”åé•·åº¦ï¼Œé¿å…éé•·
        if len(safe_name) > 100:
            safe_name = safe_name[:100]
            self.logger.info(f"æª”åéé•·ï¼Œå·²æˆªæ–·è‡³ 100 å­—å…ƒ")

        filename = f"{workflow['id']}_{safe_name}.json"

        workflows_dir = self.git_repo_path / 'workflows'
        workflows_dir.mkdir(parents=True, exist_ok=True)

        filepath = workflows_dir / filename

        # ç°¡å–®æ··æ·†æ•æ„Ÿè³‡è¨Šå¾Œå„²å­˜
        sanitized_workflow = self.sanitize_workflow(workflow)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(sanitized_workflow, f, indent=2, ensure_ascii=False)

        return filepath
    
    def git_commit_and_push(self, changed_workflows: List[str]) -> bool:
        """æäº¤è®Šæ›´åˆ° Gitï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰"""
        try:
            self.logger.info("=" * 50)
            self.logger.info("é–‹å§‹åŸ·è¡Œ Git æ“ä½œ")

            # ç¢ºä¿åœ¨ git repo ç›®éŒ„ï¼ŒåŸ·è¡Œ git add
            self.logger.info(f"åŸ·è¡Œ: git add . (åœ¨ç›®éŒ„: {self.git_repo_path})")
            result = subprocess.run(['git', 'add', '.'],
                         cwd=self.git_repo_path, check=True,
                         capture_output=True, text=True, encoding='utf-8')

            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
            self.logger.info("æª¢æŸ¥ git status...")
            status = subprocess.run(['git', 'status', '--porcelain'],
                                  cwd=self.git_repo_path, check=True,
                                  capture_output=True, text=True, encoding='utf-8')

            if not status.stdout.strip():
                self.logger.info("æ²’æœ‰éœ€è¦æäº¤çš„è®Šæ›´")
                return True

            self.logger.info(f"åµæ¸¬åˆ°è®Šæ›´æª”æ¡ˆ:\n{status.stdout}")

            # å»ºç«‹ commit message
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            commit_msg = f"[è‡ªå‹•å‚™ä»½] {timestamp}\n\nè®Šæ›´çš„å·¥ä½œæµç¨‹:\n"
            commit_msg += "\n".join(f"- {name}" for name in changed_workflows)

            # åŸ·è¡Œ commit
            self.logger.info("åŸ·è¡Œ: git commit")
            subprocess.run(['git', 'commit', '-m', commit_msg],
                         cwd=self.git_repo_path, check=True,
                         capture_output=True, text=True, encoding='utf-8')
            self.logger.info("âœ“ Commit æˆåŠŸ")

            # åŸ·è¡Œ pushï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰
            max_push_retries = 3
            for retry in range(max_push_retries):
                try:
                    self.logger.info(f"åŸ·è¡Œ: git push (å˜—è©¦ {retry + 1}/{max_push_retries})")
                    push_result = subprocess.run(['git', 'push'],
                                 cwd=self.git_repo_path, check=True,
                                 capture_output=True, text=True, encoding='utf-8')

                    if push_result.stdout:
                        self.logger.info(f"Push è¼¸å‡º: {push_result.stdout}")
                    if push_result.stderr:
                        self.logger.info(f"Push è¨Šæ¯: {push_result.stderr}")

                    self.logger.info(f"âœ“ æˆåŠŸæäº¤ä¸¦æ¨é€ {len(changed_workflows)} å€‹å·¥ä½œæµç¨‹åˆ° Git")
                    self.logger.info("=" * 50)
                    return True

                except subprocess.CalledProcessError as e:
                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ pushï¼Œéœ€è¦è¨­å®š upstream
                    if 'no upstream branch' in e.stderr:
                        self.logger.info("åµæ¸¬åˆ°é¦–æ¬¡æ¨é€ï¼ŒåŸ·è¡Œ: git push --set-upstream origin main")
                        push_result = subprocess.run(['git', 'push', '--set-upstream', 'origin', 'main'],
                                     cwd=self.git_repo_path, check=True,
                                     capture_output=True, text=True, encoding='utf-8')
                        self.logger.info("âœ“ æˆåŠŸæ¨é€åˆ°é ç«¯")
                        return True

                    # å¦‚æœè¢«æ‹’çµ•ï¼ˆé ç«¯æœ‰æ›´æ–°ï¼‰ï¼Œå…ˆ pull å† push
                    elif 'rejected' in e.stderr or 'fetch first' in e.stderr:
                        self.logger.warning("âš ï¸ æ¨é€è¢«æ‹’çµ•ï¼Œé ç«¯æœ‰æ›´æ–°")
                        self.logger.info("åŸ·è¡Œ: git pull (ä½¿ç”¨ merge ç­–ç•¥ï¼Œè¡çªæ™‚å„ªå…ˆæ¡ç”¨é ç«¯ç‰ˆæœ¬)")

                        try:
                            # ä½¿ç”¨ merge ç­–ç•¥ï¼Œè¡çªæ™‚è‡ªå‹•é¸æ“‡é ç«¯ç‰ˆæœ¬
                            pull_result = subprocess.run([
                                'git', 'pull', '--no-rebase',
                                '-X', 'theirs',  # è¡çªæ™‚é¸æ“‡é ç«¯ç‰ˆæœ¬
                                'origin', 'main'
                            ], cwd=self.git_repo_path, check=True,
                               capture_output=True, text=True, encoding='utf-8')

                            self.logger.info("âœ“ æˆåŠŸæ‹‰å–ä¸¦åˆä½µé ç«¯è®Šæ›´")

                            # ç¹¼çºŒä¸‹ä¸€è¼ªé‡è©¦
                            continue

                        except subprocess.CalledProcessError as pull_error:
                            self.logger.error(f"âœ— Pull å¤±æ•—: {pull_error.stderr}")

                            # å¦‚æœ merge ä¹Ÿå¤±æ•—ï¼Œå˜—è©¦é‡ç½®åˆ°é ç«¯ç‹€æ…‹
                            self.logger.warning("âš ï¸ å˜—è©¦é‡ç½®åˆ°é ç«¯æœ€æ–°ç‹€æ…‹")
                            try:
                                subprocess.run(['git', 'fetch', 'origin', 'main'],
                                             cwd=self.git_repo_path, check=True,
                                             capture_output=True, text=True, encoding='utf-8')
                                subprocess.run(['git', 'reset', '--hard', 'origin/main'],
                                             cwd=self.git_repo_path, check=True,
                                             capture_output=True, text=True, encoding='utf-8')
                                self.logger.info("âœ“ å·²é‡ç½®åˆ°é ç«¯æœ€æ–°ç‹€æ…‹")
                                return False  # æœ¬æ¬¡æ¨é€æ”¾æ£„ï¼Œä¸‹æ¬¡æœƒé‡æ–°å‚™ä»½
                            except subprocess.CalledProcessError:
                                self.logger.error("âœ— ç„¡æ³•é‡ç½®åˆ°é ç«¯ç‹€æ…‹")
                                return False

                    # å…¶ä»–éŒ¯èª¤
                    elif retry < max_push_retries - 1:
                        wait_time = 2 ** retry  # æŒ‡æ•¸é€€é¿: 1s, 2s, 4s
                        self.logger.warning(f"Push å¤±æ•—ï¼Œ{wait_time} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                    else:
                        # æœ€å¾Œä¸€æ¬¡é‡è©¦å¤±æ•—
                        raise

            # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
            self.logger.error("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ¨é€å¤±æ•—")
            return False

        except subprocess.CalledProcessError as e:
            self.logger.error("=" * 50)
            self.logger.error(f"âœ— Git æ“ä½œå¤±æ•—")
            self.logger.error(f"æŒ‡ä»¤: {e.cmd}")
            self.logger.error(f"è¿”å›ç¢¼: {e.returncode}")
            if e.stdout:
                self.logger.error(f"æ¨™æº–è¼¸å‡º: {e.stdout}")
            if e.stderr:
                self.logger.error(f"éŒ¯èª¤è¼¸å‡º: {e.stderr}")
            self.logger.error("=" * 50)
            return False
        except Exception as e:
            self.logger.error(f"âœ— Git æ“ä½œç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            return False
    
    def backup_workflows(self) -> Dict:
        """åŸ·è¡Œå·¥ä½œæµç¨‹å‚™ä»½"""
        self.logger.info("=" * 50)
        self.logger.info("é–‹å§‹å‚™ä»½å·¥ä½œæµç¨‹")

        result = {
            'success': False,
            'changed_count': 0,
            'total_count': 0,
            'changed_workflows': [],
            'workflow_changes': {},  # æ–°å¢ï¼šå„²å­˜æ¯å€‹ workflow çš„è®Šæ›´è©³æƒ…
            'error': None
        }

        # å–å¾—æ‰€æœ‰å·¥ä½œæµç¨‹
        workflows = self.get_all_workflows()
        if workflows is None:
            result['error'] = 'ç„¡æ³•å–å¾—å·¥ä½œæµç¨‹åˆ—è¡¨'
            return result

        result['total_count'] = len(workflows)

        # è¼‰å…¥ä¸Šæ¬¡çš„ hash å’Œå®Œæ•´è³‡æ–™
        hash_file = self.git_repo_path / '.workflow_hashes.json'
        data_file = self.git_repo_path / '.workflow_data.json'
        old_hashes = {}
        old_workflows = {}

        if hash_file.exists():
            with open(hash_file, 'r', encoding='utf-8') as f:
                old_hashes = json.load(f)

        if data_file.exists():
            with open(data_file, 'r', encoding='utf-8') as f:
                old_workflows = json.load(f)

        new_hashes = {}
        new_workflows = {}
        changed_workflows = []

        # è™•ç†æ¯å€‹å·¥ä½œæµç¨‹
        for workflow in workflows:
            detail = self.get_workflow_detail(workflow['id'])
            if detail is None:
                continue

            # è¨ˆç®— hash
            current_hash = self.calculate_hash(detail)
            new_hashes[workflow['id']] = current_hash
            new_workflows[workflow['id']] = detail

            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
            if workflow['id'] not in old_hashes or old_hashes[workflow['id']] != current_hash:
                workflow_name = workflow['name']
                self.logger.info(f"åµæ¸¬åˆ°è®Šæ›´: {workflow_name} (ID: {workflow['id']})")

                # åˆ†æè®Šæ›´ï¼ˆå¦‚æœæœ‰èˆŠç‰ˆæœ¬ï¼‰
                if workflow['id'] in old_workflows:
                    changes = self._analyze_workflow_changes(old_workflows[workflow['id']], detail)
                    change_summary = self._format_change_summary(changes)
                    self.logger.info(f"  {change_summary}")
                    result['workflow_changes'][workflow_name] = change_summary
                else:
                    # æ–°å»ºç«‹çš„ workflow
                    result['workflow_changes'][workflow_name] = "ğŸ†• æ–°å»ºç«‹çš„å·¥ä½œæµç¨‹"
                    self.logger.info(f"  ğŸ†• æ–°å»ºç«‹çš„å·¥ä½œæµç¨‹")

                self.save_workflow(detail)
                changed_workflows.append(workflow_name)

        # å„²å­˜æ–°çš„ hash å’Œè³‡æ–™
        with open(hash_file, 'w', encoding='utf-8') as f:
            json.dump(new_hashes, f, indent=2)

        # å„²å­˜ workflow è³‡æ–™æ™‚ä¹Ÿè¦ sanitizeï¼ˆé¿å…æ•æ„Ÿè³‡è¨Šå¤–æ´©ï¼‰
        sanitized_workflows = {}
        for workflow_id, workflow_data in new_workflows.items():
            sanitized_workflows[workflow_id] = self.sanitize_workflow(workflow_data)

        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(sanitized_workflows, f, indent=2, ensure_ascii=False)

        result['changed_count'] = len(changed_workflows)
        result['changed_workflows'] = changed_workflows

        # å¦‚æœæœ‰è®Šæ›´,æäº¤åˆ° Git
        if changed_workflows:
            if self.git_commit_and_push(changed_workflows):
                result['success'] = True
            else:
                result['error'] = 'Git æäº¤å¤±æ•—'
        else:
            self.logger.info("æ²’æœ‰åµæ¸¬åˆ°å·¥ä½œæµç¨‹è®Šæ›´")
            result['success'] = True

        return result
    
    def send_webhook_notification(self, data: Dict):
        """ç™¼é€ Webhook é€šçŸ¥ (Slack, Discord, Teams, etc.)"""
        webhook_config = self.notifications.get('webhook', {})
        if not webhook_config.get('enabled', False):
            return

        try:
            # æ ¹æ“šä¸åŒå¹³å°æ ¼å¼åŒ–è¨Šæ¯
            platform = webhook_config.get('platform', 'generic')

            if platform == 'slack':
                payload = {
                    'text': data.get('message', ''),
                    'blocks': [
                        {
                            'type': 'section',
                            'text': {'type': 'mrkdwn', 'text': data.get('message', '')}
                        }
                    ]
                }
            elif platform == 'discord':
                payload = {
                    'content': data.get('message', ''),
                    'embeds': [{
                        'title': data.get('title', 'n8n ç›£æ§é€šçŸ¥'),
                        'description': data.get('message', ''),
                        'color': 15158332 if data.get('status') == 'error' else 3066993
                    }]
                }
            elif platform == 'teams':
                # å»ºç«‹å®Œæ•´çš„ Adaptive Card çµæ§‹
                payload = self._create_teams_card(data)
            else:  # generic
                payload = data

            response = requests.post(
                webhook_config['url'],
                json=payload,
                timeout=10
            )
            response.raise_for_status()
            self.logger.info("âœ“ Webhook é€šçŸ¥å·²ç™¼é€")

        except Exception as e:
            self.logger.error(f"ç™¼é€ Webhook å¤±æ•—: {e}")

    def _create_teams_card(self, data: Dict) -> Dict:
        """å‰µå»º Microsoft Teams Adaptive Card"""
        status = data.get('status', 'info')
        title = data.get('title', 'n8n ç›£æ§é€šçŸ¥')

        # æ ¹æ“šç‹€æ…‹è¨­å®šé¡è‰²å’Œåœ–ç¤º
        if status == 'error':
            color = 'Attention'  # ç´…è‰²
            icon = 'âš ï¸'
        elif status == 'success':
            color = 'Good'  # ç¶ è‰²
            icon = 'âœ…'
        else:
            color = 'Default'  # ç°è‰²
            icon = 'â„¹ï¸'

        # åŸºæœ¬å¡ç‰‡çµæ§‹
        card = {
            "type": "message",
            "attachments": [
                {
                    "contentType": "application/vnd.microsoft.card.adaptive",
                    "content": {
                        "type": "AdaptiveCard",
                        "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                        "version": "1.4",
                        "body": [
                            {
                                "type": "TextBlock",
                                "text": f"{icon} {title}",
                                "size": "Large",
                                "weight": "Bolder",
                                "color": color
                            }
                        ]
                    }
                }
            ]
        }

        body = card["attachments"][0]["content"]["body"]

        # æ ¹æ“šä¸åŒçš„é€šçŸ¥é¡å‹æ·»åŠ å…§å®¹
        if 'backup_result' in data:
            # å‚™ä»½å®Œæˆé€šçŸ¥
            result = data['backup_result']
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # æ·»åŠ æ™‚é–“å’Œçµ±è¨ˆè³‡è¨Š
            body.append({
                "type": "FactSet",
                "facts": [
                    {"title": "â° å‚™ä»½æ™‚é–“", "value": timestamp},
                    {"title": "ğŸ“Š ç¸½æµç¨‹æ•¸", "value": str(result.get('total_count', 0))},
                    {"title": "âœï¸ æœ¬æ¬¡è®Šæ›´", "value": str(result.get('changed_count', 0))}
                ]
            })

            # å¦‚æœæœ‰è®Šæ›´ï¼Œé¡¯ç¤ºè®Šæ›´åˆ—è¡¨å’Œè©³æƒ…
            if result.get('changed_workflows'):
                body.append({
                    "type": "TextBlock",
                    "text": "**è®Šæ›´çš„å·¥ä½œæµç¨‹ï¼š**",
                    "weight": "Bolder",
                    "spacing": "Medium"
                })

                workflow_changes = result.get('workflow_changes', {})
                for workflow_name in result['changed_workflows']:
                    # é¡¯ç¤ºå·¥ä½œæµç¨‹åç¨±
                    body.append({
                        "type": "TextBlock",
                        "text": f"ğŸ“ **{workflow_name}**",
                        "spacing": "Small",
                        "weight": "Bolder"
                    })

                    # é¡¯ç¤ºè®Šæ›´æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
                    if workflow_name in workflow_changes:
                        change_summary = workflow_changes[workflow_name]
                        # å°‡å¤šè¡Œæ‘˜è¦åˆ†é–‹é¡¯ç¤º
                        for line in change_summary.split('\n'):
                            if line.strip():
                                body.append({
                                    "type": "TextBlock",
                                    "text": f"  {line.strip()}",
                                    "spacing": "None",
                                    "size": "Small",
                                    "isSubtle": True,
                                    "wrap": True
                                })

            # æ·»åŠ é€£çµæŒ‰éˆ•
            card["attachments"][0]["content"]["actions"] = [
                {
                    "type": "Action.OpenUrl",
                    "title": "é–‹å•Ÿ n8n",
                    "url": self.n8n_url
                },
                {
                    "type": "Action.OpenUrl",
                    "title": "æŸ¥çœ‹å‚™ä»½",
                    "url": "https://github.com/guyu1010/wanin_n8n_bk_data"
                }
            ]

        elif 'health_status' in data:
            # å¥åº·ç‹€æ…‹è®Šæ›´é€šçŸ¥
            health = data['health_status']

            body.append({
                "type": "FactSet",
                "facts": [
                    {"title": "â° æ™‚é–“", "value": health.get('timestamp', '')},
                    {"title": "ğŸ“ ç‹€æ…‹", "value": health.get('status', 'unknown')}
                ]
            })

            # å¦‚æœæœ‰éŒ¯èª¤è¨Šæ¯
            if 'error' in health:
                body.append({
                    "type": "TextBlock",
                    "text": f"**éŒ¯èª¤è¨Šæ¯ï¼š**\n{health['error']}",
                    "wrap": True,
                    "spacing": "Medium",
                    "color": "Attention"
                })

            # æ·»åŠ é€£çµæŒ‰éˆ•
            card["attachments"][0]["content"]["actions"] = [
                {
                    "type": "Action.OpenUrl",
                    "title": "æª¢æŸ¥ n8n",
                    "url": self.n8n_url
                }
            ]
        else:
            # ä¸€èˆ¬è¨Šæ¯
            message = data.get('message', '')
            body.append({
                "type": "TextBlock",
                "text": message,
                "wrap": True
            })

        return card

    def handle_health_change(self, health_status: Dict):
        """è™•ç†å¥åº·ç‹€æ…‹è®Šæ›´"""
        current_status = health_status['status']

        # å¦‚æœç‹€æ…‹æ”¹è®Š,ç™¼é€é€šçŸ¥
        if self.last_health_status != current_status:
            if current_status != 'healthy':
                # n8n å‡ºç¾å•é¡Œ
                self.logger.error(f"n8n æœå‹™ç•°å¸¸: {current_status}")
                self.send_webhook_notification({
                    'title': 'n8n æœå‹™ç•°å¸¸',
                    'status': 'error',
                    'health_status': health_status
                })
            else:
                # n8n æ¢å¾©æ­£å¸¸
                self.logger.info("n8n æœå‹™å·²æ¢å¾©æ­£å¸¸")
                self.send_webhook_notification({
                    'title': 'n8n æœå‹™æ¢å¾©',
                    'status': 'success',
                    'health_status': health_status
                })

            self.last_health_status = current_status
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´çš„ç›£æ§èˆ‡å‚™ä»½æµç¨‹"""
        self.logger.info("é–‹å§‹åŸ·è¡Œ n8n ç›£æ§èˆ‡å‚™ä»½")
        
        # 1. æª¢æŸ¥å¥åº·ç‹€æ…‹
        health_status = self.check_health()
        self.handle_health_change(health_status)
        
        # 2. å¦‚æœ n8n æ­£å¸¸,åŸ·è¡Œå‚™ä»½
        if health_status['status'] == 'healthy':
            backup_result = self.backup_workflows()

            if backup_result['changed_count'] > 0:
                self.send_webhook_notification({
                    'title': 'n8n å·¥ä½œæµç¨‹å‚™ä»½å®Œæˆ',
                    'status': 'success',
                    'backup_result': backup_result
                })
        else:
            self.logger.warning("ç”±æ–¼ n8n æœå‹™ç•°å¸¸,è·³éå‚™ä»½ä½œæ¥­")
        
        self.logger.info("ç›£æ§èˆ‡å‚™ä»½æµç¨‹çµæŸ")
        self.logger.info("=" * 50)

    def run_scheduled(self):
        """åŸ·è¡Œæ’ç¨‹æ¨¡å¼ - åœ¨æ¯å°æ™‚çš„ 00 åˆ†å’Œ 30 åˆ†åŸ·è¡Œ"""
        run_on_startup = self.schedule_config.get('run_on_startup', True)

        self.logger.info("=" * 50)
        self.logger.info("ğŸš€ n8n ç›£æ§ç³»çµ±å•Ÿå‹•ï¼ˆæ’ç¨‹æ¨¡å¼ï¼‰")
        self.logger.info("â±ï¸  åŸ·è¡Œæ™‚é–“: æ¯å°æ™‚çš„ 00 åˆ†å’Œ 30 åˆ†")
        self.logger.info(f"ğŸ”„ å•Ÿå‹•æ™‚åŸ·è¡Œ: {'æ˜¯' if run_on_startup else 'å¦'}")
        self.logger.info("=" * 50)

        # å¦‚æœè¨­å®šç‚ºå•Ÿå‹•æ™‚åŸ·è¡Œï¼Œç«‹å³åŸ·è¡Œä¸€æ¬¡
        if run_on_startup:
            self.logger.info("âš¡ ç«‹å³åŸ·è¡Œç¬¬ä¸€æ¬¡ç›£æ§...")
            try:
                self.run()
            except KeyboardInterrupt:
                raise
            except Exception as e:
                self.logger.error(f"åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # é€²å…¥æ’ç¨‹å¾ªç’°
        try:
            while True:
                # è¨ˆç®—ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“ï¼ˆæ¯å°æ™‚çš„ 00 åˆ†æˆ– 30 åˆ†ï¼‰
                now = datetime.now()
                next_run = now.replace(second=0, microsecond=0)

                # æ±ºå®šä¸‹ä¸€å€‹åŸ·è¡Œæ™‚é–“é»
                if now.minute < 30:
                    # ä¸‹ä¸€å€‹åŸ·è¡Œæ™‚é–“æ˜¯æœ¬å°æ™‚çš„ 30 åˆ†
                    next_run = next_run.replace(minute=30)
                else:
                    # ä¸‹ä¸€å€‹åŸ·è¡Œæ™‚é–“æ˜¯ä¸‹ä¸€å°æ™‚çš„ 00 åˆ†
                    next_run = next_run.replace(minute=0)
                    next_run = next_run + timedelta(hours=1)

                # å¦‚æœè¨ˆç®—å‡ºçš„æ™‚é–“å·²ç¶“éå»ï¼ˆå¯èƒ½å‰›å¥½åœ¨æ•´é»æˆ–åŠé»ï¼‰ï¼Œå‰‡è·³åˆ°ä¸‹ä¸€å€‹æ™‚é–“é»
                if next_run <= now:
                    if next_run.minute == 0:
                        next_run = next_run.replace(minute=30)
                    else:
                        next_run = next_run.replace(minute=0) + timedelta(hours=1)

                # è¨ˆç®—éœ€è¦ç­‰å¾…çš„ç§’æ•¸
                wait_seconds = (next_run - datetime.now()).total_seconds()

                self.logger.info(f"â° ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“: {next_run.strftime('%Y-%m-%d %H:%M:%S')} (ç­‰å¾… {int(wait_seconds)} ç§’)")
                time.sleep(wait_seconds)

                # åŸ·è¡Œç›£æ§èˆ‡å‚™ä»½
                try:
                    self.run()
                except KeyboardInterrupt:
                    raise
                except Exception as e:
                    self.logger.error(f"åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        except KeyboardInterrupt:
            self.logger.info("\n")
            self.logger.info("=" * 50)
            self.logger.info("â›” æ”¶åˆ°ä¸­æ–·è¨Šè™Ÿï¼Œæ­£åœ¨åœæ­¢ç›£æ§ç³»çµ±...")
            self.logger.info("=" * 50)

if __name__ == '__main__':
    import sys

    monitor = N8nMonitor('config.json')

    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨æ’ç¨‹æ¨¡å¼
    if monitor.schedule_config.get('enabled', False):
        monitor.run_scheduled()
    else:
        # å–®æ¬¡åŸ·è¡Œæ¨¡å¼ï¼ˆå…¼å®¹èˆŠç‰ˆä½¿ç”¨æ–¹å¼ï¼‰
        monitor.run()